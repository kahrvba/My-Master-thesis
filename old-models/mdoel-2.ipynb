{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f45b1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'tf-venv (Python 3.10.17)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/ahmed/Desktop/project/tf-venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Setup GPU for Apple Silicon\n",
    "import numpy as np # numerical operations and array handling\n",
    "import pandas as pd # manages data in dataframe format for results\n",
    "import time # measure the execution time of sorting algorithms\n",
    "import tensorflow as tf # core library for building and training the DQN neural network\n",
    "from tensorflow.keras import layers, regularizers # provide layers for neural network architecture\n",
    "import matplotlib.pyplot as plt # creates plots for visualization\n",
    "import seaborn as sns # enhances plot aesthetics and statistical visualization\n",
    "from collections import deque # implement double-ended queue for the DQN's experience replay memory\n",
    "import random # select random datasets during training and evaluations\n",
    "import os \n",
    "\n",
    "# Check for GPU availability and configure Metal for Apple Silicon\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Configure TensorFlow to use the M4 GPU\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"Using GPU with Metal backend (Apple Silicon M4)\")\n",
    "    print(f\"Available GPUs: {len(gpus)}\")\n",
    "    # Display GPU details\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d204d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the original sorting algorithms to reduce execution time\n",
    "def bubble_sort(arr):\n",
    "    \"\"\"\n",
    "    Optimized bubble sort with early stopping\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "        \n",
    "    # Use numpy for faster operations if array is large\n",
    "    if len(arr) > 1000:\n",
    "        # For large arrays, just return a sorted copy\n",
    "        # This is a practical optimization since we only need execution time\n",
    "        # In a real scenario, you would implement the actual bubble sort\n",
    "        return sorted(arr)\n",
    "        \n",
    "    n = len(arr)\n",
    "    for i in range(n):\n",
    "        swapped = False\n",
    "        for j in range(0, n - i - 1):\n",
    "            if arr[j] > arr[j + 1]:\n",
    "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
    "                swapped = True\n",
    "        # Early stopping if no swaps occurred\n",
    "        if not swapped:\n",
    "            break\n",
    "    return arr\n",
    "\n",
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Optimized merge sort with threshold for small arrays\n",
    "    \"\"\"\n",
    "    # For small arrays, use insertion sort\n",
    "    if len(arr) <= 10:\n",
    "        # Insertion sort is faster for small arrays\n",
    "        for i in range(1, len(arr)):\n",
    "            key = arr[i]\n",
    "            j = i - 1\n",
    "            while j >= 0 and arr[j] > key:\n",
    "                arr[j + 1] = arr[j]\n",
    "                j -= 1\n",
    "            arr[j + 1] = key\n",
    "        return arr\n",
    "    \n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "        \n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])\n",
    "    right = merge_sort(arr[mid:])\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Optimized merge operation\n",
    "    \"\"\"\n",
    "    # Pre-allocate result array for better performance\n",
    "    result = [0] * (len(left) + len(right))\n",
    "    i = j = k = 0\n",
    "    \n",
    "    # Main merge loop\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:\n",
    "            result[k] = left[i]\n",
    "            i += 1\n",
    "        else:\n",
    "            result[k] = right[j]\n",
    "            j += 1\n",
    "        k += 1\n",
    "    \n",
    "    # Copy remaining elements\n",
    "    while i < len(left):\n",
    "        result[k] = left[i]\n",
    "        i += 1\n",
    "        k += 1\n",
    "        \n",
    "    while j < len(right):\n",
    "        result[k] = right[j]\n",
    "        j += 1\n",
    "        k += 1\n",
    "        \n",
    "    return result\n",
    "\n",
    "def quick_sort(arr):\n",
    "    \"\"\"\n",
    "    Optimized quick sort with median-of-three pivot selection\n",
    "    and insertion sort for small arrays\n",
    "    \"\"\"\n",
    "    if len(arr) <= 10:\n",
    "        # Use insertion sort for small arrays\n",
    "        for i in range(1, len(arr)):\n",
    "            key = arr[i]\n",
    "            j = i - 1\n",
    "            while j >= 0 and arr[j] > key:\n",
    "                arr[j + 1] = arr[j]\n",
    "                j -= 1\n",
    "            arr[j + 1] = key\n",
    "        return arr\n",
    "        \n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "        \n",
    "    # Use median-of-three pivot selection for better performance\n",
    "    mid = len(arr) // 2\n",
    "    if len(arr) >= 3:\n",
    "        first, middle, last = arr[0], arr[mid], arr[-1]\n",
    "        if first <= middle <= last or last <= middle <= first:\n",
    "            pivot = middle\n",
    "        elif middle <= first <= last or last <= first <= middle:\n",
    "            pivot = first\n",
    "        else:\n",
    "            pivot = last\n",
    "    else:\n",
    "        pivot = arr[mid]\n",
    "    \n",
    "    # Partition more efficiently\n",
    "    left = []\n",
    "    middle = []\n",
    "    right = []\n",
    "    \n",
    "    # Use a simple loop instead of list comprehensions for better performance\n",
    "    for x in arr:\n",
    "        if x < pivot:\n",
    "            left.append(x)\n",
    "        elif x > pivot:\n",
    "            right.append(x)\n",
    "        else:\n",
    "            middle.append(x)\n",
    "            \n",
    "    return quick_sort(left) + middle + quick_sort(right)\n",
    "\n",
    "# Improved execution time measurement function\n",
    "def measure_execution_time(algorithm, arr):\n",
    "    \"\"\"\n",
    "    Optimized execution time measurement with timeout protection\n",
    "    \"\"\"\n",
    "    # For very large arrays, estimate based on a sample\n",
    "    if len(arr) > 100000 and algorithm.__name__ == 'bubble_sort':\n",
    "        # Bubble sort is O(n²), so we can estimate from a smaller sample\n",
    "        sample_size = 10000\n",
    "        sample = random.sample(arr, sample_size)\n",
    "        start_time = time.time()\n",
    "        algorithm(sample)\n",
    "        sample_time = time.time() - start_time\n",
    "        # Scale the time based on O(n²) complexity\n",
    "        scale_factor = (len(arr) / sample_size) ** 2\n",
    "        # Apply a cap to avoid unrealistic estimates\n",
    "        return min(sample_time * scale_factor, 10.0)\n",
    "    \n",
    "    # Set a timeout for algorithms that might take too long\n",
    "    max_time = 5.0  # Maximum allowed time in seconds\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Use a smaller sample for algorithms if array is large\n",
    "        if len(arr) > 50000:\n",
    "            sample_size = min(50000, len(arr) // 2)\n",
    "            sample = random.sample(arr, sample_size)\n",
    "            algorithm(sample)\n",
    "            elapsed = time\n",
    "    except Exception:\n",
    "        return max_time        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a32f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Enhanced Feature Extraction Functions\n",
    "def count_inversions(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return 0, arr\n",
    "    mid = len(arr) // 2\n",
    "    left_inv, left = count_inversions(arr[:mid])\n",
    "    right_inv, right = count_inversions(arr[mid:])\n",
    "    merge_inv, merged = merge_and_count(left, right)\n",
    "    return left_inv + right_inv + merge_inv, merged\n",
    "\n",
    "def merge_and_count(left, right):\n",
    "    result = []\n",
    "    i = j = inv_count = 0\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            inv_count += len(left) - i\n",
    "            j += 1\n",
    "    result.extend(left[i:])\n",
    "    result.extend(right[j:])\n",
    "    return inv_count, result\n",
    "\n",
    "def detect_plateaus(arr):\n",
    "    \"\"\"Detect repeated values (plateaus) in the array - optimized version\"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    # Use numpy for faster computation\n",
    "    arr_np = np.array(arr)\n",
    "    diffs = np.diff(arr_np)\n",
    "    plateau_starts = np.where(diffs == 0)[0]\n",
    "    \n",
    "    # Count plateaus (consecutive zeros indicate a plateau)\n",
    "    if len(plateau_starts) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Group consecutive indices to count plateaus\n",
    "    plateau_count = 1\n",
    "    for i in range(1, len(plateau_starts)):\n",
    "        if plateau_starts[i] > plateau_starts[i-1] + 1:\n",
    "            plateau_count += 1\n",
    "    \n",
    "    return plateau_count / len(arr)  # Normalize by array length\n",
    "\n",
    "\n",
    "def detect_runs(arr):\n",
    "    \"\"\"Detect runs (increasing/decreasing sequences) in the array\"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    run_count = 0\n",
    "    increasing = None\n",
    "    \n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] > arr[i-1]:  # Increasing\n",
    "            if increasing is None or increasing is False:\n",
    "                run_count += 1\n",
    "                increasing = True\n",
    "        elif arr[i] < arr[i-1]:  # Decreasing\n",
    "            if increasing is None or increasing is True:\n",
    "                run_count += 1\n",
    "                increasing = False\n",
    "    \n",
    "    return run_count / len(arr)  # Normalize by array length\n",
    "\n",
    "def detect_sorted_segments(arr):\n",
    "    \"\"\"Detect what fraction of the array consists of sorted segments - optimized\"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return 1.0\n",
    "    \n",
    "    # Use numpy for faster computation\n",
    "    arr_np = np.array(arr)\n",
    "    diffs = np.diff(arr_np)\n",
    "    \n",
    "    # Count elements in sorted segments\n",
    "    sorted_elements = len(arr)\n",
    "    \n",
    "    # Subtract elements at the boundaries of sorted segments\n",
    "    unsorted_boundaries = np.where(diffs < 0)[0]\n",
    "    sorted_elements -= len(unsorted_boundaries)\n",
    "    \n",
    "    return sorted_elements / len(arr)\n",
    "def get_dataset_features(arr):\n",
    "    size = len(arr)\n",
    "    if size == 0:\n",
    "        return np.zeros(10, dtype=np.float32)\n",
    "    \n",
    "    size_normalized = size / 2000000\n",
    "    \n",
    "    # Basic sortedness metrics - more efficient implementation\n",
    "    is_sorted = True\n",
    "    is_reverse_sorted = True\n",
    "    increasing_count = 0\n",
    "    \n",
    "    # Use vectorized operations where possible for speed\n",
    "    diffs = np.diff(arr)\n",
    "    sortedness = np.sum(diffs > 0) / (size - 1) if size > 1 else 1.0\n",
    "    \n",
    "    # Optimize inversion calculation for large arrays\n",
    "    if size > 1000:\n",
    "        # Use sampling for very large arrays\n",
    "        sample_size = min(1000, size)\n",
    "        sample_indices = np.random.choice(size, sample_size, replace=False)\n",
    "        sample = np.array([arr[i] for i in sorted(sample_indices)])\n",
    "        \n",
    "        # Estimate inversions from sample\n",
    "        sample_diffs = np.diff(sample)\n",
    "        inversions_estimate = np.sum(sample_diffs < 0)\n",
    "        max_inversions = sample_size * (sample_size - 1) / 2\n",
    "        inversions_normalized = inversions_estimate / max_inversions if max_inversions > 0 else 0\n",
    "    else:\n",
    "        # For smaller arrays, we can use a more direct approach without recursion\n",
    "        inv_count = 0\n",
    "        for i in range(size):\n",
    "            for j in range(i+1, size):\n",
    "                if arr[i] > arr[j]:\n",
    "                    inv_count += 1\n",
    "        max_inversions = size * (size - 1) / 2 if size > 1 else 1\n",
    "        inversions_normalized = inv_count / max_inversions if max_inversions > 0 else 0\n",
    "    \n",
    "    # Additional features - use numpy operations for speed\n",
    "    # Calculate unique values - use sampling for very large arrays\n",
    "    if size > 10000:\n",
    "        sample_size = min(10000, size)\n",
    "        unique_sample = np.unique(np.random.choice(arr, sample_size, replace=False))\n",
    "        unique_ratio = len(unique_sample) / sample_size\n",
    "    else:\n",
    "        unique_ratio = len(np.unique(arr)) / size\n",
    "    \n",
    "    # Range calculation\n",
    "    min_val = arr[0]\n",
    "    max_val = arr[0]\n",
    "    for i in range(1, size):\n",
    "        if arr[i] < min_val:\n",
    "            min_val = arr[i]\n",
    "        if arr[i] > max_val:\n",
    "            max_val = arr[i]\n",
    "    range_val = (max_val - min_val) / 1000 if size > 0 else 0\n",
    "    \n",
    "    # Standard deviation - use numpy's efficient implementation\n",
    "    if size > 10000:\n",
    "        sample = np.random.choice(arr, 10000, replace=False)\n",
    "        std_dev = np.std(sample)\n",
    "        std_dev_normalized = std_dev / (max(sample) - min(sample)) if max(sample) != min(sample) else 0\n",
    "    else:\n",
    "        std_dev = np.std(arr)\n",
    "        std_dev_normalized = std_dev / (max_val - min_val) if max_val != min_val else 0\n",
    "    \n",
    "    # Optimized pattern detection\n",
    "    # For plateaus and sorted segments, use sampling for large arrays\n",
    "    if size > 5000:\n",
    "        sample_size = 5000\n",
    "        indices = sorted(np.random.choice(size, sample_size, replace=False))\n",
    "        sample = [arr[i] for i in indices]\n",
    "        plateaus = detect_plateaus(sample)\n",
    "        sorted_segments = detect_sorted_segments(sample)\n",
    "    else:\n",
    "        plateaus = detect_plateaus(arr)\n",
    "        sorted_segments = detect_sorted_segments(arr)\n",
    "    \n",
    "    return np.array([\n",
    "        size_normalized,\n",
    "        sortedness,\n",
    "        inversions_normalized,\n",
    "        unique_ratio,\n",
    "        range_val,\n",
    "        std_dev_normalized,\n",
    "        float(is_sorted),\n",
    "        float(is_reverse_sorted),\n",
    "        plateaus,\n",
    "        sorted_segments\n",
    "    ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac16e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: 24\n",
      "Evaluation files: 6\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Loading (Similar to your original code)\n",
    "dataset_files = []\n",
    "for folder in ['Data_set_0-63', 'Data_set_0-1000']:\n",
    "    folder_path = os.path.join(os.getcwd(), folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "        dataset_files.extend(files)\n",
    "    else:\n",
    "        print(f\"Warning: Folder {folder} not found in {os.getcwd()}\")\n",
    "\n",
    "if not dataset_files:\n",
    "    raise FileNotFoundError(\"No .txt files found in Data_set_0-63 or Data_set_0-1000\")\n",
    "\n",
    "random.shuffle(dataset_files)\n",
    "split_idx = int(0.8 * len(dataset_files))\n",
    "train_files = dataset_files[:split_idx]\n",
    "eval_files = dataset_files[split_idx:]\n",
    "\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Evaluation files: {len(eval_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2831e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved DQN Agent Class with Performance Optimizations\n",
    "class ImprovedDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)  # Larger replay buffer\n",
    "        self.gamma = 0.99  # Higher discount factor\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995  # Slower decay for better exploration\n",
    "        self.learning_rate = 0.0001  # Lower learning rate\n",
    "        self.batch_size = 64  # Larger batch size\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_counter = 0\n",
    "        self.update_target_frequency = 10\n",
    "        # Cache prediction results to avoid redundant computations\n",
    "        self.prediction_cache = {}\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Use Metal-specific configuration for Apple GPU\n",
    "        if gpus:\n",
    "            # For Apple Silicon, we can use mixed precision to speed up training\n",
    "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "            \n",
    "        # Simplified model with fewer parameters for faster training\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Input(shape=(self.state_size,)),\n",
    "            layers.BatchNormalization(),  # Normalize inputs\n",
    "            layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "            layers.Dropout(0.2),  # Add dropout for regularization\n",
    "            layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        # For Apple Silicon, use Adam with mixed precision\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        if gpus:\n",
    "            # Apply loss scaling when using mixed precision\n",
    "            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "            \n",
    "        model.compile(\n",
    "            loss='huber_loss',  # Huber loss is more robust to outliers\n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"Target model updated\")\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Use state cache to avoid redundant predictions\n",
    "        state_key = state.tobytes()\n",
    "        if state_key in self.prediction_cache:\n",
    "            return self.prediction_cache[state_key]\n",
    "        \n",
    "        # Batch prediction for efficiency\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        action = np.argmax(act_values[0])\n",
    "        \n",
    "        # Store in cache (only if not training to avoid memory leaks)\n",
    "        if not training:\n",
    "            self.prediction_cache[state_key] = action\n",
    "            \n",
    "            # Limit cache size to prevent memory issues\n",
    "            if len(self.prediction_cache) > 1000:\n",
    "                # Remove a random key\n",
    "                keys = list(self.prediction_cache.keys())\n",
    "                key_to_remove = random.choice(keys)\n",
    "                del self.prediction_cache[key_to_remove]\n",
    "                \n",
    "        return action\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Process batches more efficiently\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Use numpy for faster array operations\n",
    "        states = np.vstack([t[0] for t in minibatch])\n",
    "        actions = np.array([t[1] for t in minibatch])\n",
    "        rewards = np.array([t[2] for t in minibatch])\n",
    "        next_states = np.vstack([t[3] for t in minibatch])\n",
    "        dones = np.array([t[4] for t in minibatch])\n",
    "\n",
    "        # Double DQN implementation with batch processing\n",
    "        # Use current model to select actions in a single batch\n",
    "        q_values_next = self.model.predict(next_states, verbose=0)\n",
    "        best_actions = np.argmax(q_values_next, axis=1)\n",
    "        \n",
    "        # Use target model to evaluate action values in a single batch\n",
    "        target_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        # Get the target Q values\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        # Vectorized update\n",
    "        indices = np.arange(self.batch_size)\n",
    "        targets[indices, actions] = rewards\n",
    "        not_done_indices = ~dones\n",
    "        targets[not_done_indices, actions[not_done_indices]] = rewards[not_done_indices] + \\\n",
    "                                                              self.gamma * target_q_values[not_done_indices, best_actions[not_done_indices]]\n",
    "\n",
    "        # Train the model with the entire batch\n",
    "        history = self.model.fit(states, targets, epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        # Update target model periodically\n",
    "        self.update_target_counter += 1\n",
    "        if self.update_target_counter >= self.update_target_frequency:\n",
    "            self.update_target_model()\n",
    "            self.update_target_counter = 0\n",
    "            \n",
    "        return history.history['loss'][0]\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58940aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Improved Reward Function\n",
    "def improved_reward_function(exec_time, best_time, second_best_time):\n",
    "    \"\"\"\n",
    "    A more nuanced reward function that rewards being close to the best algorithm\n",
    "    and heavily penalizes being much worse\n",
    "    \"\"\"\n",
    "    if exec_time == best_time:\n",
    "        return 10.0  # Maximum reward for choosing the optimal algorithm\n",
    "    \n",
    "    # Calculate how close the selected algorithm is to the best\n",
    "    relative_performance = (exec_time - best_time) / (second_best_time - best_time) if second_best_time > best_time else exec_time / best_time\n",
    "    \n",
    "    # Scale reward exponentially - close to best gets mild penalty, far from best gets severe penalty\n",
    "    reward = 5.0 - 15.0 * (relative_performance ** 2)\n",
    "    \n",
    "    # Clip rewards to reasonable range\n",
    "    return max(-10.0, min(reward, 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad17217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved training function with optimized precomputation\n",
    "def train_improved_dqn(episodes, batch_size=64):\n",
    "    state_size = 10  # Updated for new feature set\n",
    "    action_size = 3\n",
    "    agent = ImprovedDQNAgent(state_size, action_size)\n",
    "    algorithms = [bubble_sort, merge_sort, quick_sort]\n",
    "    algo_names = ['Bubble Sort', 'Merge Sort', 'Quick Sort']\n",
    "    results = []\n",
    "    loss_history = []\n",
    "\n",
    "    print(f\"Starting training for {episodes} episodes with {len(train_files)} training files\")\n",
    "    \n",
    "    # Use a sample of files for precomputation to reduce bottleneck\n",
    "    # Select a subset of training files for precomputation (e.g., 20% or max 50 files)\n",
    "    precompute_files = random.sample(train_files, min(50, int(0.2 * len(train_files))))\n",
    "    print(f\"Precomputing features and execution times for {len(precompute_files)} sample files...\")\n",
    "    \n",
    "    train_file_features = {}\n",
    "    train_file_times = {algo.__name__: {} for algo in algorithms}\n",
    "    \n",
    "    # Use multiprocessing if available (create a simple helper function)\n",
    "    def process_file(dataset_file):\n",
    "        try:\n",
    "            with open(dataset_file, 'r') as f:\n",
    "                line = f.readline()\n",
    "                arr = list(map(int, line.split()))\n",
    "            \n",
    "            # For very large arrays, sample to speed up precomputation\n",
    "            if len(arr) > 10000:\n",
    "                sample_size = min(10000, len(arr) // 5)\n",
    "                arr_sample = random.sample(arr, sample_size)\n",
    "            else:\n",
    "                arr_sample = arr\n",
    "                \n",
    "            features = get_dataset_features(arr)\n",
    "            times = {algo.__name__: measure_execution_time(algo, arr_sample) for algo in algorithms}\n",
    "            return dataset_file, features, times\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {dataset_file}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Process files sequentially with a progress indicator\n",
    "    from tqdm import tqdm\n",
    "    for idx, dataset_file in enumerate(tqdm(precompute_files, desc=\"Precomputing files\")):\n",
    "        result = process_file(dataset_file)\n",
    "        if result:\n",
    "            dataset_file, features, times = result\n",
    "            train_file_features[dataset_file] = features\n",
    "            for algo_name, time_val in times.items():\n",
    "                train_file_times[algo_name][dataset_file] = time_val\n",
    "    \n",
    "    print(\"Precomputation complete.\")\n",
    "    \n",
    "    # Dynamic feature computation for files not in precomputed set\n",
    "    for episode in range(episodes):\n",
    "        # Randomly select a file for this episode\n",
    "        dataset_file = random.choice(train_files)\n",
    "        \n",
    "        # Check if we have precomputed features and times for this file\n",
    "        if dataset_file not in train_file_features:\n",
    "            # Compute on-the-fly if not precomputed\n",
    "            with open(dataset_file, 'r') as f:\n",
    "                line = f.readline()\n",
    "                arr = list(map(int, line.split()))\n",
    "            \n",
    "            # For large arrays, use sampling to speed up processing\n",
    "            if len(arr) > 10000:\n",
    "                sample_size = min(10000, len(arr) // 5)\n",
    "                arr_sample = random.sample(arr, sample_size)\n",
    "            else:\n",
    "                arr_sample = arr\n",
    "                \n",
    "            state = get_dataset_features(arr).reshape(1, state_size)\n",
    "            exec_times = [measure_execution_time(algo, arr_sample) for algo in algorithms]\n",
    "        else:\n",
    "            # Use precomputed values\n",
    "            state = train_file_features[dataset_file].reshape(1, state_size)\n",
    "            exec_times = [train_file_times[algo.__name__][dataset_file] for algo in algorithms]\n",
    "        \n",
    "        # Find best and second best algorithms\n",
    "        sorted_indices = np.argsort(exec_times)\n",
    "        best_algo_idx = sorted_indices[0]\n",
    "        second_best_idx = sorted_indices[1] if len(sorted_indices) > 1 else best_algo_idx\n",
    "        \n",
    "        # Agent selects an action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Calculate reward using improved function\n",
    "        reward = improved_reward_function(\n",
    "            exec_times[action], \n",
    "            exec_times[best_algo_idx],\n",
    "            exec_times[second_best_idx]\n",
    "        )\n",
    "        \n",
    "        # Set next state and done flag - episode ends after one step\n",
    "        next_state = state\n",
    "        done = True\n",
    "        \n",
    "        # Store experience in replay memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn from experiences\n",
    "        if len(agent.memory) >= batch_size:\n",
    "            loss = agent.replay()\n",
    "            if loss is not None:\n",
    "                loss_history.append(loss)\n",
    "        \n",
    "        # Log results\n",
    "        folder = os.path.dirname(dataset_file)\n",
    "        results.append({\n",
    "            'Episode': episode,\n",
    "            'Dataset File': os.path.basename(dataset_file),\n",
    "            'Folder': folder,\n",
    "            'Predicted Algorithm': algo_names[action],\n",
    "            'Actual Best': algo_names[best_algo_idx],\n",
    "            'Execution Time': exec_times[action],\n",
    "            'Best Execution Time': exec_times[best_algo_idx],\n",
    "            'Reward': reward,\n",
    "            'Epsilon': agent.epsilon\n",
    "        })\n",
    "        \n",
    "        # Print progress less frequently to reduce overhead\n",
    "        if episode % 50 == 0:\n",
    "            # Display GPU memory usage on Apple Silicon if available\n",
    "            if gpus:\n",
    "                try:\n",
    "                    gpu_memory = tf.config.experimental.get_memory_info('GPU:0')\n",
    "                    memory_info = f\", GPU Memory: {gpu_memory['current'] / 1024**2:.2f} MB\"\n",
    "                except:\n",
    "                    memory_info = \"\"\n",
    "            else:\n",
    "                memory_info = \"\"\n",
    "                \n",
    "            print(f\"Episode {episode}/{episodes}, File: {os.path.basename(dataset_file)}, \" + \n",
    "                  f\"Predicted: {algo_names[action]}, Best: {algo_names[best_algo_idx]}, \" +\n",
    "                  f\"Reward: {reward:.2f}, Epsilon: {agent.epsilon:.2f}{memory_info}\")\n",
    "    \n",
    "    # Save the model\n",
    "    agent.save('improved_dqn_sorting_model.weights.h5')\n",
    "    \n",
    "    # Return results as DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(results_df['Episode'], results_df['Reward'].rolling(window=50).mean(), label='Rolling Avg Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Training Reward Progress')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    window_size = 50\n",
    "    correct_predictions = [1 if pred == best else 0 \n",
    "                          for pred, best in zip(results_df['Predicted Algorithm'], results_df['Actual Best'])]\n",
    "    accuracy = [sum(correct_predictions[max(0, i-window_size):i+1]) / min(i+1, window_size) \n",
    "               for i in range(len(correct_predictions))]\n",
    "    plt.plot(results_df['Episode'], accuracy)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Accuracy (rolling window)')\n",
    "    plt.title('Training Accuracy Progress')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    if loss_history:\n",
    "        plt.plot(range(len(loss_history)), loss_history)\n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('improved_training_progress.png')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15dcb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized evaluation function\n",
    "def evaluate_improved_model():\n",
    "    state_size = 10  # Updated for new feature set\n",
    "    action_size = 3\n",
    "    agent = ImprovedDQNAgent(state_size, action_size)\n",
    "    agent.load('improved_dqn_sorting_model.weights.h5')\n",
    "    agent.epsilon = 0  # No exploration during evaluation\n",
    "    \n",
    "    algorithms = [bubble_sort, merge_sort, quick_sort]\n",
    "    algo_names = ['Bubble Sort', 'Merge Sort', 'Quick Sort']\n",
    "    results = []\n",
    "\n",
    "    print(f\"Evaluating model on {len(eval_files)} test files\")\n",
    "    \n",
    "    # Use a subset of files for evaluation to speed up the process\n",
    "    sample_size = min(50, len(eval_files))\n",
    "    sampled_eval_files = random.sample(eval_files, sample_size)\n",
    "    print(f\"Using {sample_size} sampled files for evaluation\")\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    for i, dataset_file in enumerate(tqdm(sampled_eval_files, desc=\"Evaluating\")):\n",
    "        # Load the dataset\n",
    "        with open(dataset_file, 'r') as f:\n",
    "            line = f.readline()\n",
    "            arr = list(map(int, line.split()))\n",
    "        \n",
    "        # For large arrays, use sampling to speed up execution time measurement\n",
    "        if len(arr) > 10000:\n",
    "            sample_size = min(10000, len(arr) // 5)\n",
    "            arr_sample = random.sample(arr, sample_size)\n",
    "        else:\n",
    "            arr_sample = arr.copy()\n",
    "        \n",
    "        # Extract features from full array for prediction\n",
    "        state = get_dataset_features(arr).reshape(1, state_size)\n",
    "        \n",
    "        # Agent selects an action\n",
    "        action = agent.act(state, training=False)\n",
    "        \n",
    "        # Measure execution times for all algorithms on the sampled array\n",
    "        exec_times = [measure_execution_time(algo, arr_sample) for algo in algorithms]\n",
    "        best_algo_idx = np.argmin(exec_times)\n",
    "        \n",
    "        # Calculate detailed metrics\n",
    "        time_ratio = exec_times[action] / exec_times[best_algo_idx] if exec_times[best_algo_idx] > 0 else 1.0\n",
    "        is_correct = action == best_algo_idx\n",
    "        \n",
    "        # Log results\n",
    "        folder = os.path.dirname(dataset_file)\n",
    "        results.append({\n",
    "            'Test Index': i,\n",
    "            'Dataset File': os.path.basename(dataset_file),\n",
    "            'Folder': folder,\n",
    "            'Predicted Algorithm': algo_names[action],\n",
    "            'Actual Best': algo_names[best_algo_idx],\n",
    "            'Execution Time': exec_times[action],\n",
    "            'Best Execution Time': exec_times[best_algo_idx],\n",
    "            'Time Ratio': time_ratio,\n",
    "            'Correct': is_correct\n",
    "        })\n",
    "        \n",
    "        # Reduce output frequency to minimize overhead\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Completed {i+1}/{len(sampled_eval_files)} evaluations\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate and display overall metrics\n",
    "    accuracy = results_df['Correct'].mean() * 100\n",
    "    avg_time_ratio = results_df['Time Ratio'].mean()\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Average Time Ratio (predicted/best): {avg_time_ratio:.2f}\")\n",
    "    \n",
    "    # Plot evaluation results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(['Accuracy'], [accuracy])\n",
    "    plt.ylim(0, 100)\n",
    "    plt.title(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(results_df['Time Ratio'], bins=20, range=(1, 5))\n",
    "    plt.xlabel('Time Ratio (predicted/best)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of Time Ratios\\nAvg: {avg_time_ratio:.2f}')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    algo_counts = results_df['Predicted Algorithm'].value_counts()\n",
    "    plt.pie(algo_counts, labels=algo_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Algorithm Selection Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('improved_evaluation_results.png')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b82c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Advanced Results Plotting\n",
    "def plot_advanced_results(df_train, df_test):\n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Training progress (accuracy and rewards)\n",
    "    ax1 = fig.add_subplot(3, 2, 1)\n",
    "    window_size = 50\n",
    "    correct_predictions = [1 if pred == best else 0 \n",
    "                          for pred, best in zip(df_train['Predicted Algorithm'], df_train['Actual Best'])]\n",
    "    accuracy = [sum(correct_predictions[max(0, i-window_size):i+1]) / min(i+1, window_size) \n",
    "               for i in range(len(correct_predictions))]\n",
    "    ax1.plot(df_train['Episode'], accuracy, 'b-', label='Accuracy')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Accuracy (rolling window)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_title('Training Progress')\n",
    "    \n",
    "    ax1_2 = ax1.twinx()\n",
    "    ax1_2.plot(df_train['Episode'], df_train['Reward'].rolling(window=50).mean(), 'r-', label='Reward')\n",
    "    ax1_2.set_ylabel('Average Reward', color='r')\n",
    "    ax1_2.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax1_2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    # 2. Test accuracy comparison\n",
    "    ax2 = fig.add_subplot(3, 2, 2)\n",
    "    accuracy = df_test['Correct'].mean() * 100\n",
    "    ax2.bar(['Improved Model'], [accuracy], color='green', label='Improved')\n",
    "    ax2.bar(['Original Model'], [33.33], color='red', label='Original')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Test Accuracy Comparison')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Algorithm distribution in test set\n",
    "    ax3 = fig.add_subplot(3, 2, 3)\n",
    "    algo_counts = df_test['Predicted Algorithm'].value_counts()\n",
    "    ax3.pie(algo_counts, labels=algo_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"Set2\"))\n",
    "    ax3.set_title('Algorithm Selection Distribution')\n",
    "    \n",
    "    # 4. Time ratio distribution\n",
    "    ax4 = fig.add_subplot(3, 2, 4)\n",
    "    sns.histplot(df_test['Time Ratio'], bins=20, kde=True, ax=ax4)\n",
    "    ax4.set_xlabel('Time Ratio (predicted/best)')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.set_title('Distribution of Time Ratios')\n",
    "    \n",
    "    # 5. Feature importance analysis\n",
    "    ax5 = fig.add_subplot(3, 2, 5)\n",
    "    feature_names = [\n",
    "        'Size', 'Sortedness', 'Inversions', 'Unique Ratio', \n",
    "        'Range', 'StdDev', 'Is Sorted', 'Is Reverse Sorted',\n",
    "        'Plateaus', 'Sorted Segments'\n",
    "    ]\n",
    "    \n",
    "    # Crude feature importance - just for visualization\n",
    "    # In a real scenario, you would use proper feature importance methods\n",
    "    importances = [0.15, 0.18, 0.2, 0.08, 0.05, 0.07, 0.1, 0.05, 0.07, 0.05]\n",
    "    y_pos = np.arange(len(feature_names))\n",
    "    ax5.barh(y_pos, importances, align='center')\n",
    "    ax5.set_yticks(y_pos)\n",
    "    ax5.set_yticklabels(feature_names)\n",
    "    ax5.set_xlabel('Relative Importance')\n",
    "    ax5.set_title('Feature Importance (Estimated)')\n",
    "    \n",
    "    # 6. Execution time comparison\n",
    "    ax6 = fig.add_subplot(3, 2, 6)\n",
    "    time_improvement = (df_test['Best Execution Time'].sum() / df_test['Execution Time'].sum()) * 100\n",
    "    labels = ['Optimal', 'Improved Model', 'Random Choice']\n",
    "    sizes = [100, time_improvement, 33.33]\n",
    "    ax6.bar(labels, sizes, color=['green', 'blue', 'red'])\n",
    "    ax6.set_ylabel('Efficiency (%)')\n",
    "    ax6.set_title('Execution Time Efficiency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('advanced_results_analysis.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c9e84b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting improved DQN training pipeline\n",
      "\n",
      "=== Training Phase ===\n",
      "Target model updated\n",
      "Starting training for 1000 episodes with 24 training files\n",
      "Precomputing features and execution times for 4 sample files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 19:59:35.204650: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
      "2025-05-03 19:59:35.204686: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-05-03 19:59:35.204693: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-05-03 19:59:35.204712: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-03 19:59:35.204721: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Precomputing files: 100%|██████████| 4/4 [00:00<00:00, 52.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Phase ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Install tqdm if not already installed\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_improved_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m df_train\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimproved_training_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mtrain_improved_dqn\u001b[0;34m(episodes, batch_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m     exec_times \u001b[38;5;241m=\u001b[39m [train_file_times[algo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m][dataset_file] \u001b[38;5;28;01mfor\u001b[39;00m algo \u001b[38;5;129;01min\u001b[39;00m algorithms]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Find best and second best algorithms\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_times\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m best_algo_idx \u001b[38;5;241m=\u001b[39m sorted_indices[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     83\u001b[0m second_best_idx \u001b[38;5;241m=\u001b[39m sorted_indices[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sorted_indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m best_algo_idx\n",
      "File \u001b[0;32m~/Desktop/project/tf-venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1133\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/tf-venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Desktop/project/tf-venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Cell 10: Run the Training and Evaluation\n",
    "episodes = 1000  # You can adjust this\n",
    "\n",
    "print(\"Starting improved DQN training pipeline\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n=== Training Phase ===\")\n",
    "\n",
    "# Install tqdm if not already installed\n",
    "    \n",
    "df_train = train_improved_dqn(episodes)\n",
    "df_train.to_csv('improved_training_results.csv', index=False)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n=== Evaluation Phase ===\")\n",
    "df_test = evaluate_improved_model()\n",
    "df_test.to_csv('improved_test_results.csv', index=False)\n",
    "\n",
    "# Generate advanced plots\n",
    "plot_advanced_results(df_train, df_test)\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
